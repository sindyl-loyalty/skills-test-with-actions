# This YAML file defines a github action that we want to use to run DBT Tests
# in Databricks in a safe environment.

# To learn more about YAML, see:
# https://learnxinyminutes.com/docs/yaml/

# To see the reference for the Github Actions, see:
# https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions

name: DBT Testing

on:
  # We want to have two ways of triggering the action.
  # The first will be a manual trigger that is more appropriate for
  # manual testing. This way, we can only trigger the action
  # when we want while we are building it out, and we won't
  # trigger it involuntarily when doing other things in this repo.

  workflow_dispatch:
    # TODO: If we need a variable here for a manual run, we can define it.
    # This can be accessed in the Job as ${{inputs.example}}
    inputs:
      example:
        description: 'This is just a Dummy Input for Manual Runs'
        required: false
        type: string
      branch:
        description: |
          The branch to check out for running DBT tests.
          By default, the workflow check out the branch that the workflow is run on (chosen from drop down).
        required: false
        type: string

  # TODO: Once we are happy with the manual runs, we will need to define the github
  # or git actions that will trigger this Github Action .

  #Workflows will not run on pull_request activity if the pull request has a merge conflict.
  #By default, a workflow only runs when a pull_request event's activity type is opened, synchronize, or reopened.
#  pull_request:
#    types:
#      - opened
#      - synchronize
#    branches:
#      - main
#    paths:
#      - 'dp_airflow/dbt/**'

# TODO: Should we have concurrency setting to control how many actions can run in parallel?

# A Job is a set of steps (actions) that run on a specific runner (virtual computer).
# If you have more than one job, they run in parallel by default. See:
# https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions#jobs

jobs:
  dbt-test:
    name: DBT Test
    # Ubuntu Latest has heaps of software installed on it. See:
    # https://github.com/actions/runner-images/blob/ubuntu22/20240804.1/images/ubuntu/Ubuntu2204-Readme.md
    runs-on: ubuntu-latest

    #Specify location of the profiles.yml configuration file and project directory
    #And name of the secret containing the token to connect to Databricks
    env:
      DBT_PROFILES_DIR: ./dp_airflow/dbt/data_platform
      DBT_PROJECT_DIR: ./dp_airflow/dbt/data_platform
      CLONE_PY_DIR: ./dp_airflow/dbt/data_platform/databricks_cloning_for_automated_testing.py
      DATABRICKS_HOST_RED: ${{ secrets.databricks_host_red }}
      DATABRICKS_TOKEN_RED: ${{ secrets.databricks_token_red }}
      databricks_token: ${{ secrets.databricks_token }}

    # Each job has Steps - each step has its own process in the runner, so variables are not shared. See:
    # https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions#jobsjob_idsteps
    steps:
      - name: Echo the Input Just For a Start
        run: |
          echo This is the skeleton of the CICD Task. Our Input is: ${{inputs.example}}

    # TODO: Lets have a step to checkout the code in the branch we want to update.
    #  (how do we do this with manual input?)
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          ref: ${{ inputs.branch || github.event.pull_request.head.ref || github.ref }}
      
      - name: Get checked out branch name
        run: |
          echo "branch_name=$(git branch --show-current)" >> $GITHUB_ENV
          
    # TODO: How can we detect which models have changed? 
      - name: Fetch main branch
        if: ${{ env.branch_name != 'main' }}
        run: |
          git fetch origin main:main
    
    # Comparing files between the checked out branch and main branch 
    # TODO: We may also want to run test for updated tests and macros etc
      - name: Get changed models
        run: |
          echo Checked out branch: ${{ env.branch_name }}
          
          changed_models=$(git diff --name-only main -- "$DBT_PROJECT_DIR/models" | grep '.sql$')
          models=""
          while read -r model; do
          model_name=$(basename "$model" .sql)
          models="$models $model_name"
          done <<< "$changed_models"
          echo "changed_models=$models" >> $GITHUB_ENV  

          echo Changed models: "$models"
    
    # TODO: Lets have a step which installs DBT
      - name: Choose python version
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dbt and databricks adapter
        run: |
          python -m pip install dbt-core dbt-databricks

      - name: Install dbt package dependencies
        run: |
          dbt --version
          dbt deps --target local --profiles-dir "$DBT_PROFILES_DIR" --project-dir "$DBT_PROJECT_DIR"

    # # TODO: Can we use that code to connect to Databricks?
    # # DBT handles connections to Databricks using the credentials provided in the profiles.yml file
    #   - name: Check databricks connection
    #     run: |
    #       dbt debug --target predeployment --profiles-dir "$DBT_PROFILES_DIR" --project-dir "$DBT_PROJECT_DIR"

    # TODO: How can we clone tables using the Databricks APIs for the tests we will run?
    # It seems that to early exit the job without failing the job, an if condition has to be added to all of the steps or steps have to be moved to another job 
      # - name: Databricks cloning from prod to qual
      #   if: ${{ env.changed_models != '' }}
      #   run: |
      #     python ${{ env.CLONE_PY_DIR }}

    # TODO: How can we run the DBT tests for the models that are updated?
    # TODO: Should we run the DBT tests for downstream tables on clones?
      - name: Run tests for changed and downstream models on clones
        if: ${{ env.changed_models != '' }}
        run: |
          for model in ${{ env.changed_models }}; do
          dbt test --target local -m "$model+"
          done

    # TODO: How can we clean up the cloned tables we have created?
    # A new workflow that run the Databricks cloning step should create or replace the cloned tables 

    # # TODO: How can we fail the github action if the tests fail?
    # DBT tests fail should fail the Run DBT tests step and the workflow
